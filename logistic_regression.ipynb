{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandramouli224/python_code/blob/main/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "e_eVb1HfJbfT",
        "outputId": "fe0c7ff0-a54d-4fa2-e626-8b76e39a7441"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"Untitled4.ipynb\r\n",
        "\r\n",
        "Automatically generated by Colaboratory.\r\n",
        "\r\n",
        "Original file is located at\r\n",
        "    https://colab.research.google.com/drive/1N633elJqKFZEDmRi9Q4oi0OcqPG_0X8v\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.datasets import make_blobs\r\n",
        "\r\n",
        "class Logistic_Regression:\r\n",
        "  def __init__(self, alpha=0.01,precision=1e-07, maxIter=100000):\r\n",
        "    self.alpha = alpha\r\n",
        "    self.precision = precision\r\n",
        "    self.maxIter = maxIter\r\n",
        "\r\n",
        "\r\n",
        "  def get_unique(self,y):\r\n",
        "    return np.unique(y)\r\n",
        "\r\n",
        "\r\n",
        "  def multivariant_train(self,X,y):\r\n",
        "    self.theta = []\r\n",
        "    self.cost = []\r\n",
        "    X= np.insert(X,0,1,axis=1)\r\n",
        "    for i in self.get_unique(y):\r\n",
        "      self.y1 = np.where(y == i, 1, 0)\r\n",
        "      theta = np.zeros(X.shape[1])\r\n",
        "      cost = []\r\n",
        "      itr = 0\r\n",
        "      theta_deff=theta+1\r\n",
        "      while abs(theta_deff).all()>self.precision and itr<self.maxIter:\r\n",
        "      #for itr in range(self.maxIter):\r\n",
        "        z = X.dot(theta)\r\n",
        "        theta_old=theta\r\n",
        "        hypothesis = self.sigmoidFunction(z)\r\n",
        "        theta, cost1 = self.gradiant_function(X,hypothesis,theta,self.y1)\r\n",
        "        theta_deff =theta_old-theta\r\n",
        "        cost.append(cost1)\r\n",
        "        itr +=1\r\n",
        "      self.theta.append((theta, i))\r\n",
        "      self.cost.append((cost,i))\r\n",
        "    return self\r\n",
        "  \r\n",
        "\r\n",
        "  def binary_train(self,X,y):\r\n",
        "    X= np.insert(X,0,1,axis=1)\r\n",
        "    theta = np.zeros(X.shape[1])\r\n",
        "    cost = []\r\n",
        "    itr =0\r\n",
        "    theta_deff=theta+1\r\n",
        "    while theta_deff.all()>self.precision and itr<self.maxIter:\r\n",
        "      #for itr in range(self.maxIter):\r\n",
        "      z = X.dot(theta)\r\n",
        "      theta_old=theta\r\n",
        "      hypothesis = self.sigmoidFunction(z)\r\n",
        "      theta, cost1 = self.gradiant_function(X,hypothesis,theta,y)\r\n",
        "      theta_deff =theta_old-theta\r\n",
        "      cost.append(cost1)\r\n",
        "      itr +=1\r\n",
        "    self.theta = theta\r\n",
        "    self.cost = cost\r\n",
        "    return self\r\n",
        "  \r\n",
        "  def train(self, X,y):\r\n",
        "    self.m, self.n = X.shape\r\n",
        "    self.uni = self.get_unique(y)\r\n",
        "    if(len(self.get_unique(y))>2):\r\n",
        "      return self.multivariant_train(X,y)\r\n",
        "    else:\r\n",
        "      return self.binary_train(X,y)\r\n",
        "    #return self\r\n",
        "    \r\n",
        "  def predict(self, X):\r\n",
        "    x1=X\r\n",
        "    X= np.insert(X,0,1,axis=1)\r\n",
        "    X_predicted1 = []\r\n",
        "    if(len(self.uni)<=2):\r\n",
        "      X_predicted = []\r\n",
        "      for i in X:\r\n",
        "        if(self.sigmoidFunction(i.dot(self.theta))>0.5):\r\n",
        "          X_predicted.append(1)\r\n",
        "        else:\r\n",
        "          X_predicted.append(0)\r\n",
        "      return X_predicted\r\n",
        "    else:\r\n",
        "      for i in X:\r\n",
        "        predict=[]\r\n",
        "        for theta, cost in self.theta:\r\n",
        "          predict.append((self.sigmoidFunction(i.dot(theta)), cost))\r\n",
        "        X_predicted1.append(max(predict)[1])\r\n",
        "      return X_predicted1\r\n",
        "  \r\n",
        "\r\n",
        "  def sigmoidFunction(self, z):\r\n",
        "    return 1/(1 + np.exp(-z))\r\n",
        "  \r\n",
        "  \r\n",
        "  def gradiant_function(self,X,hypothesis,theta,y):\r\n",
        "    #gradient = np.dot(X.T, (hypothesis - y)) / m\r\n",
        "    gradient = X.T.dot( (hypothesis - y)) / self.m\r\n",
        "    #theta -= self.alpha * gradient\r\n",
        "    theta =theta - ( self.alpha * gradient )\r\n",
        "    cost = (1 / self.m) * (np.sum(-y.T.dot(np.log(hypothesis)) - (1 - y).T.dot(np.log(1 - hypothesis))))\r\n",
        "    return theta, cost\r\n",
        "  \r\n",
        "  def accuracy(self, X, y):\r\n",
        "    X=np.array(X)\r\n",
        "    y=np.array(y)\r\n",
        "    count=0\r\n",
        "    for i in range(len(X)):\r\n",
        "      if(X[i]==y[i]):\r\n",
        "        count +=1\r\n",
        "    score = (count / len(y))*100\r\n",
        "    #print(X)\r\n",
        "    return score\r\n",
        "  \r\n",
        "\r\n",
        "  def min_max_scaling(self,X):\r\n",
        "    Xnew=pd.DataFrame()\r\n",
        "    for col, coldata in X.iteritems():\r\n",
        "      xsub=[]\r\n",
        "      for x in coldata.values:\r\n",
        "        x = (x-min(coldata.values))/(max(coldata.values)-min(coldata.values))\r\n",
        "        xsub.append(x)\r\n",
        "      Xnew[col]=xsub\r\n",
        "    return np.array(Xnew)\r\n",
        "\r\n",
        "def binary_test():\r\n",
        "  X, y = make_blobs(n_samples=100, centers=2)\r\n",
        "  from sklearn.model_selection import train_test_split\r\n",
        "  scores=[]\r\n",
        "\r\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.50)\r\n",
        "  logi = Logistic_Regression(precision = 1e-01,  maxIter = 100).train(X_train, y_train)\r\n",
        "  predition1 = logi.predict(X_test)\r\n",
        "  print((predition1,y_test))\r\n",
        "  score1 = logi.accuracy(predition1,y_test)\r\n",
        "  print(\"the accuracy of the model for binary classification is : \",score1)\r\n",
        "\r\n",
        "  from sklearn.linear_model import LogisticRegression\r\n",
        "  lg = LogisticRegression().fit(X_train, y_train)\r\n",
        "  print(\"the accuracy of the sklearn model for binary classification is : \",lg.score(X_test,y_test)*100)\r\n",
        "\r\n",
        "\r\n",
        "def multi_class_Test():\r\n",
        "  f = pd.read_csv('beer.txt',delimiter=\"\\t\", header=None) \r\n",
        "  f.columns=['calorific_value','nitrogen', 'turbidity', 'style', 'alcohol', 'sugars', 'bitterness', 'beer_id', 'colour', 'degree_of_fermentation']\r\n",
        "  overall_accuraccy = []\r\n",
        "  overall_sklr=[]\r\n",
        "  for _ in range(0,10):\r\n",
        "    logi = Logistic_Regression()\r\n",
        "    train_data = f.sample(frac=0.667)\r\n",
        "    test_data = f.drop(train_data.index)\r\n",
        "    X_train = train_data[['calorific_value','nitrogen', 'turbidity', 'alcohol', 'sugars', 'bitterness', 'beer_id', 'colour', 'degree_of_fermentation']]\r\n",
        "    X_train = logi.min_max_scaling(X_train)\r\n",
        "    y_train = train_data['style']\r\n",
        "    X_test = test_data[['calorific_value','nitrogen', 'turbidity', 'alcohol', 'sugars', 'bitterness', 'beer_id', 'colour', 'degree_of_fermentation']]\r\n",
        "    X_test = logi.min_max_scaling(X_test)\r\n",
        "    y_test = test_data['style']\r\n",
        "    model = logi.train(X_train, y_train)\r\n",
        "    prediction1 = model.predict(X_test)\r\n",
        "    accuracy = logi.accuracy(prediction1,y_test)\r\n",
        "    overall_accuraccy.append(accuracy)\r\n",
        "    from sklearn.linear_model import LogisticRegression\r\n",
        "    from sklearn import preprocessing\r\n",
        "    lb_encoder = preprocessing.LabelEncoder()\r\n",
        "    y_train=lb_encoder.fit_transform(y_train)\r\n",
        "    lg = LogisticRegression(multi_class='ovr').fit(X_train, y_train)\r\n",
        "    y_test=lb_encoder.fit_transform(y_test)\r\n",
        "    score = lg.score(X_test,y_test)*100\r\n",
        "    overall_sklr.append(score)\r\n",
        "  print(\"Accuracy for self implemented logistic regression :\", overall_accuraccy)\r\n",
        "  print(\"Accuracy for sklearn logistic regression :\", overall_sklr)\r\n",
        "  print(\"##############################################################################################\")\r\n",
        "  print(\"avg accuracy for 10 iterations of self inplemented logistic regression model is :\", sum(overall_accuraccy)/len(overall_accuraccy))\r\n",
        "  print(\"avg accuracy for 10 iterations of self sklearn logistic regression model is :\", sum(overall_sklr)/len(overall_sklr))\r\n",
        "\r\n",
        "\r\n",
        "multi_class_Test()\r\n",
        "binary_test()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-08832cfd992c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mmulti_class_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0mbinary_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-08832cfd992c>\u001b[0m in \u001b[0;36mmulti_class_Test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mprediction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-08832cfd992c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariant_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-08832cfd992c>\u001b[0m in \u001b[0;36mmultivariant_train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtheta_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradiant_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtheta_deff\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtheta_old\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-08832cfd992c>\u001b[0m in \u001b[0;36mgradiant_function\u001b[0;34m(self, X, hypothesis, theta, y)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;31m#theta -= self.alpha * gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (103,) not aligned: 1 (dim 0) != 103 (dim 0)"
          ]
        }
      ]
    }
  ]
}