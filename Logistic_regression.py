# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N633elJqKFZEDmRi9Q4oi0OcqPG_0X8v
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

class Logistic_Regression:
  def __init__(self, alpha=0.01,precision=1e-07, maxIter=100000):
    self.alpha = alpha          #alpha is learning rate and min is 0.01
    self.precision = precision  #precision is the difference between previous cost and current cost in gradient decent. Gradient decent will stop when precision is 1e-07
    self.maxIter = maxIter      #maximum iterations allowed for gradient decent.


  def get_unique(self,y):
    return np.unique(y)            #This method will identify all classes in target variable


  def multivariant_train(self,X,y): #this method is for multi class logistic regression which is also know as 1-vs-rest implementation
    self.theta = []
    self.cost = []
    #X= np.insert(X,0,1,axis=1)
    for i in self.get_unique(y):    #cost and theta is calculated for each and every implementation is 1-vs-rest implementation
      self.y1 = np.where(y == i, 1, 0) #one class in target variable is made class 1 and rest are made class  zero for each cycle
      theta = np.zeros(X.shape[1])
      cost = []
      itr = 0
      theta_deff=theta+1
      while abs(theta_deff).all()>self.precision and itr<self.maxIter: #this while loop is carried untill the precision of all thetas is <= 1e-07 or max no of iterations reached 
        z = X.dot(theta)
        theta_old=theta
        hypothesis = self.sigmoidFunction(z)  #hypothesis (h-theta) is calculated using sigmoid method
        theta, cost1 = self.gradiant_function(X,hypothesis,theta,self.y1) #updated theta and costs are calculated and returned by  gradiant_function method
        theta_deff =theta_old-theta #theta_deff is calculated to compare with precision
        cost.append(cost1)
        itr +=1
      self.theta.append((theta, i))#theta and costs are appended to self.theta and self.cost for further use in prediction method
      self.cost.append((cost,i))
    return self
  

  def binary_train(self,X,y): #this method is implemetaion of normal logistic regression algorithm i.e.. for binary classification
    theta = np.zeros(X.shape[1])
    cost = []
    itr =0
    theta_deff=theta+1
    while theta_deff.all()>self.precision and itr<self.maxIter:   #this while loop is carried untill the precision of all thetas is <= 1e-07 or max no of iterations reached
      z = X.dot(theta)
      theta_old=theta
      hypothesis = self.sigmoidFunction(z)#hypothesis (h-theta) is calculated using sigmoid method
      theta, cost1 = self.gradiant_function(X,hypothesis,theta,y) #updated theta and costs are calculated and returned by  gradiant_function method
      theta_deff =theta_old-theta #theta_deff is calculated to compare with precision
      cost.append(cost1)
      itr +=1
    self.theta = theta #theta and costs are appended to self.theta and self.cost for further use in prediction method
    self.cost = cost
    return self
  
  def train(self, X,y): #this method will identify whether the classification task is binary or 1-vs-rest implementation and call multivariant or binary algorithms accordingly
    self.m, self.n = X.shape
    self.uni = self.get_unique(y)
    if(len(self.get_unique(y))>2): #if the classes in target  variable is more than 2 it calls multivariant_train else it will call binary_train
      return self.multivariant_train(X,y)
    elif(len(self.get_unique(y))==2):
      return self.binary_train(X,y)
    elif(len(self.get_unique(y))<2): #if classes in target variable is less than 2 then it will throw an error
      raise MyValidationError("Target variable must contain two or more classes to train the algorithm")
      return 4
    
  def predict(self, X):  #This method predicts for given x imput
    x1=X
    X_predicted1 = []
    if(len(self.uni)<=2): #checks for number of target classes in training data and calls proceeds  
      X_predicted = []
      for i in X:
        if(self.sigmoidFunction(i.dot(self.theta))>0.5):
          X_predicted.append(1) #if the predicted probability is greater than 0.5 then class 1 is assigned else class 0 is assigned 
        else:
          X_predicted.append(0)
      return X_predicted
    else:                   #if number of classes in training data is more than 2 then below code is executed
      for i in X:
        predict=[]
        for theta, cla in self.theta:  #theta and class for each iteration in 1-vs-rest is iterated andpredicted
          predict.append((self.sigmoidFunction(i.dot(theta)), cla))
        X_predicted1.append(max(predict)[1])#maximum probability out of all 1-vs-rest models is assigned
      return X_predicted1
  
  def sigmoidFunction(self, z):   #this method calculates sigmoid equation 
    return 1/(1 + np.exp(-z))
  
  def gradiant_function(self,X,hypothesis,theta,y): #this method calculates and returns cost and theta values 
    gradient = X.T.dot( (hypothesis - y)) / self.m
    theta =theta - ( self.alpha * gradient )
    cost = (1 / self.m) * (np.sum(-y.T.dot(np.log(hypothesis)) - (1 - y).T.dot(np.log(1 - hypothesis))))
    return theta, cost
  
  def accuracy_score(self, y_predict, y_test):#this method calculates precision for given y_predicted and Y_true values
    y_predict=np.array(y_predict)
    y_test=np.array(y_test)
    count=0
    for i in range(len(y_predict)):
      if(y_predict[i]==y_test[i]):
        count +=1
    score = (count / len(y_test))*100
    return score
  
  def min_max_scaling(self,X):#this method does min-max scaling
    Xnew=pd.DataFrame()
    for col, coldata in X.iteritems():
      xsub=[]
      for x in coldata.values:
        x = (x-min(coldata.values))/(max(coldata.values)-min(coldata.values))
        xsub.append(x)
      Xnew[col]=xsub
    return np.array(Xnew)

class MyValidationError(Exception): #this class if for logging errors
    pass

def binary_test():   #this method is to test binary classification algorithm
  X, y = make_blobs(n_samples=1000, centers=2)   #takes 1000 samples with 2 classes in target variable
  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.50)#splits data into 50:50 train and test sets
  logi = Logistic_Regression(precision = 1e-01,  maxIter = 100).train(X_train, y_train)     #trains LR model with precision 1e-01 and maxitr = 100
  predition1 = logi.predict(X_test) #predicts for train data
  score1 = logi.accuracy_score(predition1,y_test) #calculates accuracy for predicted values using inbuilt accuracy method
  print("the precision of the model for binary classification is : ",score1)
  from sklearn.linear_model import LogisticRegression #above steps are repeated but with sklearn LR for comparison
  lg = LogisticRegression().fit(X_train, y_train)
  print("the precision of the sklearn model for binary classification is : ",lg.score(X_test,y_test)*100) #calculates accuracy using sklearn LR score method

def multi_class_Test():    #this method is to train train and predict for given beer data with 10 iteration
  f = pd.read_csv('beer.txt',delimiter="\t", header=None) 
  f.columns=['calorific_value','nitrogen', 'turbidity', 'style', 'alcohol',
             'sugars', 'bitterness', 'beer_id', 'colour', 'degree_of_fermentation']
  y = f['style']
  X1 = f[['calorific_value','nitrogen', 'turbidity', 'alcohol','sugars', 'bitterness', 'beer_id', 'colour', 'degree_of_fermentation']] 
  X = Logistic_Regression().min_max_scaling(X1) #feature scaling is done using inbuilt min_max_scaling method
  overall_accuraccy = []
  overall_accuracy_sklr =[]
  out_put=pd.DataFrame(columns=['Actual', 'Predicted'])
  out_put.to_csv('output.csv')
  for _ in range(0,10):       #for loop for 10 iterations in which models are trained 10 times with random 2\3 train data and predictions are done for rmaining 1\3 test data
    logi = Logistic_Regression() #initializing self implemented LR class
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.33)  # 2:1 training and test sets are divinding using train_test_split() method
    model = logi.train(X_train, y_train)    #training model with train data set
    prediction1 = model.predict(X_test)     #predictions are done for test data set
    accuracy = logi.accuracy_score(prediction1,y_test)
    overall_accuraccy.append(accuracy)      #accuracy is calculated using inbult accuracy_score() method
    print("Classification report for self implemented logistic regression model is :")
    print(classification_report(y_test,prediction1)) #classification report is printed for further analysis from sklearn.metrics library
    print("confusion matrix for self implemented logistic regression model is :")
    print(confusion_matrix(y_test,prediction1))      #confusion matrix is also printed from sklearn.metrics library
    pd.DataFrame([["Iteration",_]]).to_csv("output.csv",mode='a',header=False)
    out_put = pd.DataFrame({
        "actal":y_test,
        "predicted":prediction1
    })
    out_put.to_csv("output.csv",mode='a',header=False)
    lb_encoder = preprocessing.LabelEncoder()        #labelencoder() is used to encode target variable labels
    y_train=lb_encoder.fit_transform(y_train)
    lg = LogisticRegression(multi_class='ovr').fit(X_train, y_train) #logistic regression from sklearn library is used for comparison
    y_test=lb_encoder.fit_transform(y_test)                                                         
    y_pred =lg.predict(X_test)
    y_pred = lb_encoder.inverse_transform(y_pred)
    y_test = lb_encoder.inverse_transform(y_test)
    print("Classification report skLearn logistic regression model is :")
    print(classification_report(y_test,y_pred))     #classification report is printed for further comparison
    print("confusion matrix for skLearn logistic regression model is :")
    print(confusion_matrix(y_test,y_pred))          #confusion matrix is also printed from sklearn.metrics library
    accuracy_sk = accuracy_score(y_test,y_pred)*100
    overall_accuracy_sklr.append(accuracy_sk)
    print("****************competed iteration :",_)
  print("accuracy for self implemented logistic regression :", overall_accuraccy)
  print("accuracy for sklearn logistic regression :", overall_accuracy_sklr)
  print("##############################################################################################")
  print("avg accuracy for 10 iterations of self implemented logistic regression model is :", sum(overall_accuraccy)/len(overall_accuraccy))
  print("avg accuracy for 10 iterations of self sklearn logistic regression model is :", sum(overall_accuracy_sklr)/len(overall_accuracy_sklr))

def single_class_test():         #this method tests at least two class criteria
  X, y = make_blobs(n_samples=1000, centers=1)
  logi = Logistic_Regression(precision = 1e-01,  maxIter = 100).train(X, y) 

multi_class_Test()
binary_test()
single_class_test()